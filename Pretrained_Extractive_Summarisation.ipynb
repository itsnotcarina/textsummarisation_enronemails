{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extractive Summarisation with BERT, GPT-2, Text Rank\n",
    "\n",
    "An average worker spends approximately 28 % of his working hours reading and answering emails, adding up to 11 hours a week (McKinsey, 2012). To free up time, a valuable resource, we will train different models to summarise the key message of an e-mails.\n",
    "\n",
    "To summarise the incoming emails, we will make use of extractive summarisation. There are two main forms of Text Summarization, extractive and abstractive. While extractive summarisation seeks to find the most informative sentences within a large body of text and then forms them to a summary, an abstractive summarisation model generates concise phrases that are semantically consistent with the large body of text. \n",
    "\n",
    "For simplicity we will compare various extractive summarisation techniques. To do this, we have manually summarised roughly 100 emails, such that we can compare our models with the manual summarisations in terms of accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\n",
      "         .:::.     .::.       \n",
      "        ....yy:    .yy.       \n",
      "        :.  .yy.    y.        \n",
      "             :y:   .:         \n",
      "             .yy  .:          \n",
      "              yy..:           \n",
      "              :y:.            \n",
      "              .y.             \n",
      "             .:.              \n",
      "        ....:.                \n",
      "        :::.                  \n",
      "\u001b[0;33m\n",
      "• Project files and data should be stored in /project. This is shared among everyone\n",
      "  in the project.\n",
      "• Personal files and configuration should be stored in /home/faculty.\n",
      "• Files outside /project and /home/faculty will be lost when this server is terminated.\n",
      "• Create custom environments to setup your servers reproducibly.\n",
      "\u001b[0m\n",
      "Requirement already satisfied: langdetect in /opt/anaconda/envs/Python3/lib/python3.8/site-packages (1.0.8)\n",
      "Requirement already satisfied: six in /opt/anaconda/envs/Python3/lib/python3.8/site-packages (from langdetect) (1.15.0)\n",
      "\u001b[1m\n",
      "         .:::.     .::.       \n",
      "        ....yy:    .yy.       \n",
      "        :.  .yy.    y.        \n",
      "             :y:   .:         \n",
      "             .yy  .:          \n",
      "              yy..:           \n",
      "              :y:.            \n",
      "              .y.             \n",
      "             .:.              \n",
      "        ....:.                \n",
      "        :::.                  \n",
      "\u001b[0;33m\n",
      "• Project files and data should be stored in /project. This is shared among everyone\n",
      "  in the project.\n",
      "• Personal files and configuration should be stored in /home/faculty.\n",
      "• Files outside /project and /home/faculty will be lost when this server is terminated.\n",
      "• Create custom environments to setup your servers reproducibly.\n",
      "\u001b[0m\n",
      "Requirement already satisfied: bert-extractive-summarizer in /opt/anaconda/envs/Python3/lib/python3.8/site-packages (0.7.1)\n",
      "Requirement already satisfied: transformers in /opt/anaconda/envs/Python3/lib/python3.8/site-packages (from bert-extractive-summarizer) (2.2.0)\n",
      "Requirement already satisfied: spacy in /opt/anaconda/envs/Python3/lib/python3.8/site-packages (from bert-extractive-summarizer) (2.0.12)\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda/envs/Python3/lib/python3.8/site-packages (from bert-extractive-summarizer) (0.23.1)\n",
      "Requirement already satisfied: requests in /opt/anaconda/envs/Python3/lib/python3.8/site-packages (from transformers->bert-extractive-summarizer) (2.24.0)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda/envs/Python3/lib/python3.8/site-packages (from transformers->bert-extractive-summarizer) (4.47.0)\n",
      "Requirement already satisfied: sacremoses in /opt/anaconda/envs/Python3/lib/python3.8/site-packages (from transformers->bert-extractive-summarizer) (0.0.43)\n",
      "Requirement already satisfied: sentencepiece in /opt/anaconda/envs/Python3/lib/python3.8/site-packages (from transformers->bert-extractive-summarizer) (0.1.95)\n",
      "Requirement already satisfied: regex in /opt/anaconda/envs/Python3/lib/python3.8/site-packages (from transformers->bert-extractive-summarizer) (2017.4.5)\n",
      "Requirement already satisfied: boto3 in /opt/anaconda/envs/Python3/lib/python3.8/site-packages (from transformers->bert-extractive-summarizer) (1.16.13)\n",
      "Requirement already satisfied: numpy in /opt/anaconda/envs/Python3/lib/python3.8/site-packages (from transformers->bert-extractive-summarizer) (1.18.5)\n",
      "Requirement already satisfied: cymem<1.32,>=1.30 in /opt/anaconda/envs/Python3/lib/python3.8/site-packages (from spacy->bert-extractive-summarizer) (1.31.2)\n",
      "Requirement already satisfied: murmurhash<0.29,>=0.28 in /opt/anaconda/envs/Python3/lib/python3.8/site-packages (from spacy->bert-extractive-summarizer) (0.28.0)\n",
      "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /opt/anaconda/envs/Python3/lib/python3.8/site-packages (from spacy->bert-extractive-summarizer) (0.9.6)\n",
      "Requirement already satisfied: ujson>=1.35 in /opt/anaconda/envs/Python3/lib/python3.8/site-packages (from spacy->bert-extractive-summarizer) (1.35)\n",
      "Requirement already satisfied: preshed<2.0.0,>=1.0.0 in /opt/anaconda/envs/Python3/lib/python3.8/site-packages (from spacy->bert-extractive-summarizer) (1.0.1)\n",
      "Requirement already satisfied: thinc<6.11.0,>=6.10.3 in /opt/anaconda/envs/Python3/lib/python3.8/site-packages (from spacy->bert-extractive-summarizer) (6.10.3)\n",
      "Requirement already satisfied: dill<0.3,>=0.2 in /opt/anaconda/envs/Python3/lib/python3.8/site-packages (from spacy->bert-extractive-summarizer) (0.2.9)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /opt/anaconda/envs/Python3/lib/python3.8/site-packages (from scikit-learn->bert-extractive-summarizer) (1.5.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/anaconda/envs/Python3/lib/python3.8/site-packages (from scikit-learn->bert-extractive-summarizer) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/anaconda/envs/Python3/lib/python3.8/site-packages (from scikit-learn->bert-extractive-summarizer) (0.16.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/anaconda/envs/Python3/lib/python3.8/site-packages (from requests->transformers->bert-extractive-summarizer) (1.25.9)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/anaconda/envs/Python3/lib/python3.8/site-packages (from requests->transformers->bert-extractive-summarizer) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda/envs/Python3/lib/python3.8/site-packages (from requests->transformers->bert-extractive-summarizer) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/anaconda/envs/Python3/lib/python3.8/site-packages (from requests->transformers->bert-extractive-summarizer) (3.0.4)\n",
      "Requirement already satisfied: click in /opt/anaconda/envs/Python3/lib/python3.8/site-packages (from sacremoses->transformers->bert-extractive-summarizer) (7.1.2)\n",
      "Requirement already satisfied: six in /opt/anaconda/envs/Python3/lib/python3.8/site-packages (from sacremoses->transformers->bert-extractive-summarizer) (1.15.0)\n",
      "Requirement already satisfied: botocore<1.20.0,>=1.19.13 in /opt/anaconda/envs/Python3/lib/python3.8/site-packages (from boto3->transformers->bert-extractive-summarizer) (1.19.13)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/anaconda/envs/Python3/lib/python3.8/site-packages (from boto3->transformers->bert-extractive-summarizer) (0.10.0)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /opt/anaconda/envs/Python3/lib/python3.8/site-packages (from boto3->transformers->bert-extractive-summarizer) (0.3.3)\n",
      "Requirement already satisfied: msgpack<1.0.0,>=0.5.6 in /opt/anaconda/envs/Python3/lib/python3.8/site-packages (from thinc<6.11.0,>=6.10.3->spacy->bert-extractive-summarizer) (0.6.2)\n",
      "Requirement already satisfied: msgpack-numpy<1.0.0,>=0.4.1 in /opt/anaconda/envs/Python3/lib/python3.8/site-packages (from thinc<6.11.0,>=6.10.3->spacy->bert-extractive-summarizer) (0.4.7.1)\n",
      "Requirement already satisfied: cytoolz<0.10,>=0.9.0 in /opt/anaconda/envs/Python3/lib/python3.8/site-packages (from thinc<6.11.0,>=6.10.3->spacy->bert-extractive-summarizer) (0.9.0.1)\n",
      "Requirement already satisfied: wrapt<1.11.0,>=1.10.0 in /opt/anaconda/envs/Python3/lib/python3.8/site-packages (from thinc<6.11.0,>=6.10.3->spacy->bert-extractive-summarizer) (1.10.11)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/anaconda/envs/Python3/lib/python3.8/site-packages (from botocore<1.20.0,>=1.19.13->boto3->transformers->bert-extractive-summarizer) (2.8.1)\n",
      "Requirement already satisfied: toolz>=0.8.0 in /opt/anaconda/envs/Python3/lib/python3.8/site-packages (from cytoolz<0.10,>=0.9.0->thinc<6.11.0,>=6.10.3->spacy->bert-extractive-summarizer) (0.10.0)\n",
      "\u001b[1m\n",
      "         .:::.     .::.       \n",
      "        ....yy:    .yy.       \n",
      "        :.  .yy.    y.        \n",
      "             :y:   .:         \n",
      "             .yy  .:          \n",
      "              yy..:           \n",
      "              :y:.            \n",
      "              .y.             \n",
      "             .:.              \n",
      "        ....:.                \n",
      "        :::.                  \n",
      "\u001b[0;33m\n",
      "• Project files and data should be stored in /project. This is shared among everyone\n",
      "  in the project.\n",
      "• Personal files and configuration should be stored in /home/faculty.\n",
      "• Files outside /project and /home/faculty will be lost when this server is terminated.\n",
      "• Create custom environments to setup your servers reproducibly.\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/anaconda/envs/Python3/lib/python3.8/site-packages (1.8.0)\n",
      "Requirement already satisfied: numpy in /opt/anaconda/envs/Python3/lib/python3.8/site-packages (from torch) (1.18.5)\n",
      "Requirement already satisfied: typing-extensions in /opt/anaconda/envs/Python3/lib/python3.8/site-packages (from torch) (3.7.4.2)\n",
      "\u001b[1m\n",
      "         .:::.     .::.       \n",
      "        ....yy:    .yy.       \n",
      "        :.  .yy.    y.        \n",
      "             :y:   .:         \n",
      "             .yy  .:          \n",
      "              yy..:           \n",
      "              :y:.            \n",
      "              .y.             \n",
      "             .:.              \n",
      "        ....:.                \n",
      "        :::.                  \n",
      "\u001b[0;33m\n",
      "• Project files and data should be stored in /project. This is shared among everyone\n",
      "  in the project.\n",
      "• Personal files and configuration should be stored in /home/faculty.\n",
      "• Files outside /project and /home/faculty will be lost when this server is terminated.\n",
      "• Create custom environments to setup your servers reproducibly.\n",
      "\u001b[0m\n",
      "Requirement already satisfied: rouge_score in /opt/anaconda/envs/Python3/lib/python3.8/site-packages (0.0.4)\n",
      "Requirement already satisfied: six>=1.14.0 in /opt/anaconda/envs/Python3/lib/python3.8/site-packages (from rouge_score) (1.15.0)\n",
      "Requirement already satisfied: absl-py in /opt/anaconda/envs/Python3/lib/python3.8/site-packages (from rouge_score) (0.11.0)\n",
      "Requirement already satisfied: nltk in /opt/anaconda/envs/Python3/lib/python3.8/site-packages (from rouge_score) (3.5)\n",
      "Requirement already satisfied: numpy in /opt/anaconda/envs/Python3/lib/python3.8/site-packages (from rouge_score) (1.18.5)\n",
      "Requirement already satisfied: regex in /opt/anaconda/envs/Python3/lib/python3.8/site-packages (from nltk->rouge_score) (2017.4.5)\n",
      "Requirement already satisfied: click in /opt/anaconda/envs/Python3/lib/python3.8/site-packages (from nltk->rouge_score) (7.1.2)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda/envs/Python3/lib/python3.8/site-packages (from nltk->rouge_score) (4.47.0)\n",
      "Requirement already satisfied: joblib in /opt/anaconda/envs/Python3/lib/python3.8/site-packages (from nltk->rouge_score) (0.16.0)\n",
      "\u001b[1m\n",
      "         .:::.     .::.       \n",
      "        ....yy:    .yy.       \n",
      "        :.  .yy.    y.        \n",
      "             :y:   .:         \n",
      "             .yy  .:          \n",
      "              yy..:           \n",
      "              :y:.            \n",
      "              .y.             \n",
      "             .:.              \n",
      "        ....:.                \n",
      "        :::.                  \n",
      "\u001b[0;33m\n",
      "• Project files and data should be stored in /project. This is shared among everyone\n",
      "  in the project.\n",
      "• Personal files and configuration should be stored in /home/faculty.\n",
      "• Files outside /project and /home/faculty will be lost when this server is terminated.\n",
      "• Create custom environments to setup your servers reproducibly.\n",
      "\u001b[0m\n",
      "Requirement already satisfied: transformers==2.2.0 in /opt/anaconda/envs/Python3/lib/python3.8/site-packages (2.2.0)\n",
      "Requirement already satisfied: boto3 in /opt/anaconda/envs/Python3/lib/python3.8/site-packages (from transformers==2.2.0) (1.16.13)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda/envs/Python3/lib/python3.8/site-packages (from transformers==2.2.0) (4.47.0)\n",
      "Requirement already satisfied: requests in /opt/anaconda/envs/Python3/lib/python3.8/site-packages (from transformers==2.2.0) (2.24.0)\n",
      "Requirement already satisfied: sentencepiece in /opt/anaconda/envs/Python3/lib/python3.8/site-packages (from transformers==2.2.0) (0.1.95)\n",
      "Requirement already satisfied: regex in /opt/anaconda/envs/Python3/lib/python3.8/site-packages (from transformers==2.2.0) (2017.4.5)\n",
      "Requirement already satisfied: numpy in /opt/anaconda/envs/Python3/lib/python3.8/site-packages (from transformers==2.2.0) (1.18.5)\n",
      "Requirement already satisfied: sacremoses in /opt/anaconda/envs/Python3/lib/python3.8/site-packages (from transformers==2.2.0) (0.0.43)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /opt/anaconda/envs/Python3/lib/python3.8/site-packages (from boto3->transformers==2.2.0) (0.3.3)\n",
      "Requirement already satisfied: botocore<1.20.0,>=1.19.13 in /opt/anaconda/envs/Python3/lib/python3.8/site-packages (from boto3->transformers==2.2.0) (1.19.13)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/anaconda/envs/Python3/lib/python3.8/site-packages (from boto3->transformers==2.2.0) (0.10.0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/anaconda/envs/Python3/lib/python3.8/site-packages (from requests->transformers==2.2.0) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/anaconda/envs/Python3/lib/python3.8/site-packages (from requests->transformers==2.2.0) (1.25.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda/envs/Python3/lib/python3.8/site-packages (from requests->transformers==2.2.0) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/anaconda/envs/Python3/lib/python3.8/site-packages (from requests->transformers==2.2.0) (2.10)\n",
      "Requirement already satisfied: click in /opt/anaconda/envs/Python3/lib/python3.8/site-packages (from sacremoses->transformers==2.2.0) (7.1.2)\n",
      "Requirement already satisfied: six in /opt/anaconda/envs/Python3/lib/python3.8/site-packages (from sacremoses->transformers==2.2.0) (1.15.0)\n",
      "Requirement already satisfied: joblib in /opt/anaconda/envs/Python3/lib/python3.8/site-packages (from sacremoses->transformers==2.2.0) (0.16.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/anaconda/envs/Python3/lib/python3.8/site-packages (from botocore<1.20.0,>=1.19.13->boto3->transformers==2.2.0) (2.8.1)\n",
      "\u001b[1m\n",
      "         .:::.     .::.       \n",
      "        ....yy:    .yy.       \n",
      "        :.  .yy.    y.        \n",
      "             :y:   .:         \n",
      "             .yy  .:          \n",
      "              yy..:           \n",
      "              :y:.            \n",
      "              .y.             \n",
      "             .:.              \n",
      "        ....:.                \n",
      "        :::.                  \n",
      "\u001b[0;33m\n",
      "• Project files and data should be stored in /project. This is shared among everyone\n",
      "  in the project.\n",
      "• Personal files and configuration should be stored in /home/faculty.\n",
      "• Files outside /project and /home/faculty will be lost when this server is terminated.\n",
      "• Create custom environments to setup your servers reproducibly.\n",
      "\u001b[0m\n",
      "Requirement already satisfied: spacy==2.0.12 in /opt/anaconda/envs/Python3/lib/python3.8/site-packages (2.0.12)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/anaconda/envs/Python3/lib/python3.8/site-packages (from spacy==2.0.12) (2.24.0)\n",
      "Requirement already satisfied: thinc<6.11.0,>=6.10.3 in /opt/anaconda/envs/Python3/lib/python3.8/site-packages (from spacy==2.0.12) (6.10.3)\n",
      "Requirement already satisfied: ujson>=1.35 in /opt/anaconda/envs/Python3/lib/python3.8/site-packages (from spacy==2.0.12) (1.35)\n",
      "Requirement already satisfied: regex==2017.4.5 in /opt/anaconda/envs/Python3/lib/python3.8/site-packages (from spacy==2.0.12) (2017.4.5)\n",
      "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /opt/anaconda/envs/Python3/lib/python3.8/site-packages (from spacy==2.0.12) (0.9.6)\n",
      "Requirement already satisfied: numpy>=1.7 in /opt/anaconda/envs/Python3/lib/python3.8/site-packages (from spacy==2.0.12) (1.18.5)\n",
      "Requirement already satisfied: murmurhash<0.29,>=0.28 in /opt/anaconda/envs/Python3/lib/python3.8/site-packages (from spacy==2.0.12) (0.28.0)\n",
      "Requirement already satisfied: dill<0.3,>=0.2 in /opt/anaconda/envs/Python3/lib/python3.8/site-packages (from spacy==2.0.12) (0.2.9)\n",
      "Requirement already satisfied: cymem<1.32,>=1.30 in /opt/anaconda/envs/Python3/lib/python3.8/site-packages (from spacy==2.0.12) (1.31.2)\n",
      "Requirement already satisfied: preshed<2.0.0,>=1.0.0 in /opt/anaconda/envs/Python3/lib/python3.8/site-packages (from spacy==2.0.12) (1.0.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/anaconda/envs/Python3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy==2.0.12) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/anaconda/envs/Python3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy==2.0.12) (1.25.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/anaconda/envs/Python3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy==2.0.12) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda/envs/Python3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy==2.0.12) (2020.6.20)\n",
      "Requirement already satisfied: msgpack-numpy<1.0.0,>=0.4.1 in /opt/anaconda/envs/Python3/lib/python3.8/site-packages (from thinc<6.11.0,>=6.10.3->spacy==2.0.12) (0.4.7.1)\n",
      "Requirement already satisfied: six<2.0.0,>=1.10.0 in /opt/anaconda/envs/Python3/lib/python3.8/site-packages (from thinc<6.11.0,>=6.10.3->spacy==2.0.12) (1.15.0)\n",
      "Requirement already satisfied: msgpack<1.0.0,>=0.5.6 in /opt/anaconda/envs/Python3/lib/python3.8/site-packages (from thinc<6.11.0,>=6.10.3->spacy==2.0.12) (0.6.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /opt/anaconda/envs/Python3/lib/python3.8/site-packages (from thinc<6.11.0,>=6.10.3->spacy==2.0.12) (4.47.0)\n",
      "Requirement already satisfied: cytoolz<0.10,>=0.9.0 in /opt/anaconda/envs/Python3/lib/python3.8/site-packages (from thinc<6.11.0,>=6.10.3->spacy==2.0.12) (0.9.0.1)\n",
      "Requirement already satisfied: wrapt<1.11.0,>=1.10.0 in /opt/anaconda/envs/Python3/lib/python3.8/site-packages (from thinc<6.11.0,>=6.10.3->spacy==2.0.12) (1.10.11)\n",
      "Requirement already satisfied: toolz>=0.8.0 in /opt/anaconda/envs/Python3/lib/python3.8/site-packages (from cytoolz<0.10,>=0.9.0->thinc<6.11.0,>=6.10.3->spacy==2.0.12) (0.10.0)\n"
     ]
    }
   ],
   "source": [
    "# Installing relevant libraries\n",
    "!pip install langdetect\n",
    "!pip install bert-extractive-summarizer\n",
    "!pip install torch\n",
    "!pip install rouge_score\n",
    "!pip install transformers==2.2.0\n",
    "!pip install spacy==2.0.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import langdetect\n",
    "import re\n",
    "import math\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# for BERT\n",
    "from summarizer import Summarizer\n",
    "\n",
    "# for GPT-2\n",
    "from summarizer import TransformerSummarizer\n",
    "\n",
    "# for Text Rank\n",
    "import nltk \n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize, sent_tokenize, RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import networkx as nx\n",
    "from nltk import sent_tokenize, word_tokenize, PorterStemmer\n",
    "\n",
    "# Ignoring Warnings\n",
    "pd.set_option('mode.chained_assignment', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read manually labelled data\n",
    "emails_labelled = pd.read_csv('/project/emails_labelled.csv',index_col=[0])\n",
    "\n",
    "# Formatting the dataframe \n",
    "emails_labelled = emails_labelled.reset_index()\n",
    "emails_labelled = emails_labelled.drop([\"index\",\"from\",\"to\"], axis = 1)\n",
    "\n",
    "\n",
    "# Assigning text body to BERT and GPT-2 columns for further analysis, later we can use apply to summarise on the following columns \n",
    "emails_labelled[\"Bert\"] = emails_labelled[\"body\"]\n",
    "emails_labelled[\"GPT2\"] = emails_labelled[\"body\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>subject</th>\n",
       "      <th>body</th>\n",
       "      <th>sentiment_label</th>\n",
       "      <th>summary_label</th>\n",
       "      <th>Bert</th>\n",
       "      <th>GPT2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fri, 14 Jul 2000 08:44:00 -0700 (PDT)</td>\n",
       "      <td>New Gas Transportation Product on EnronOnline</td>\n",
       "      <td>The Global Gas Pipeline group is looking to tr...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Global Gas Pipeline group is looking to trade ...</td>\n",
       "      <td>The Global Gas Pipeline group is looking to tr...</td>\n",
       "      <td>The Global Gas Pipeline group is looking to tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Wed, 6 Dec 2000 07:26:00 -0800 (PST)</td>\n",
       "      <td>Revised CalJournal Ad</td>\n",
       "      <td>IEP Team,\\nAttached is a revised January CalJo...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Revised January CalJournal ad for review</td>\n",
       "      <td>IEP Team,\\nAttached is a revised January CalJo...</td>\n",
       "      <td>IEP Team,\\nAttached is a revised January CalJo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Thu, 13 Dec 2001 16:29:38 -0800 (PST)</td>\n",
       "      <td>Staci Holtzman</td>\n",
       "      <td>\\n\\tAt your earliest convenience, please send ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>comments on Staci's performance</td>\n",
       "      <td>\\n\\tAt your earliest convenience, please send ...</td>\n",
       "      <td>\\n\\tAt your earliest convenience, please send ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tue, 30 Jan 2001 05:10:00 -0800 (PST)</td>\n",
       "      <td>Shaeffer redline</td>\n",
       "      <td>I ran a redline from the last version I had el...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Forwarded the current version to Herman for hi...</td>\n",
       "      <td>I ran a redline from the last version I had el...</td>\n",
       "      <td>I ran a redline from the last version I had el...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Thu, 30 Dec 1999 05:46:00 -0800 (PST)</td>\n",
       "      <td>Plan update</td>\n",
       "      <td>---------------------- Forwarded by Daren J Fa...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Forwarded, get copy of plan and review</td>\n",
       "      <td>---------------------- Forwarded by Daren J Fa...</td>\n",
       "      <td>---------------------- Forwarded by Daren J Fa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    date  \\\n",
       "0  Fri, 14 Jul 2000 08:44:00 -0700 (PDT)   \n",
       "1   Wed, 6 Dec 2000 07:26:00 -0800 (PST)   \n",
       "2  Thu, 13 Dec 2001 16:29:38 -0800 (PST)   \n",
       "3  Tue, 30 Jan 2001 05:10:00 -0800 (PST)   \n",
       "4  Thu, 30 Dec 1999 05:46:00 -0800 (PST)   \n",
       "\n",
       "                                         subject  \\\n",
       "0  New Gas Transportation Product on EnronOnline   \n",
       "1                          Revised CalJournal Ad   \n",
       "2                                 Staci Holtzman   \n",
       "3                               Shaeffer redline   \n",
       "4                                    Plan update   \n",
       "\n",
       "                                                body  sentiment_label  \\\n",
       "0  The Global Gas Pipeline group is looking to tr...              0.0   \n",
       "1  IEP Team,\\nAttached is a revised January CalJo...              0.0   \n",
       "2  \\n\\tAt your earliest convenience, please send ...              0.0   \n",
       "3  I ran a redline from the last version I had el...              0.0   \n",
       "4  ---------------------- Forwarded by Daren J Fa...              0.0   \n",
       "\n",
       "                                       summary_label  \\\n",
       "0  Global Gas Pipeline group is looking to trade ...   \n",
       "1           Revised January CalJournal ad for review   \n",
       "2                    comments on Staci's performance   \n",
       "3  Forwarded the current version to Herman for hi...   \n",
       "4             Forwarded, get copy of plan and review   \n",
       "\n",
       "                                                Bert  \\\n",
       "0  The Global Gas Pipeline group is looking to tr...   \n",
       "1  IEP Team,\\nAttached is a revised January CalJo...   \n",
       "2  \\n\\tAt your earliest convenience, please send ...   \n",
       "3  I ran a redline from the last version I had el...   \n",
       "4  ---------------------- Forwarded by Daren J Fa...   \n",
       "\n",
       "                                                GPT2  \n",
       "0  The Global Gas Pipeline group is looking to tr...  \n",
       "1  IEP Team,\\nAttached is a revised January CalJo...  \n",
       "2  \\n\\tAt your earliest convenience, please send ...  \n",
       "3  I ran a redline from the last version I had el...  \n",
       "4  ---------------------- Forwarded by Daren J Fa...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emails_labelled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-trained BERT\n",
    "\n",
    "We specifically make use of the BERT summarizer, a pre-trained BERT model in a wrapper function, which is already finetuned for extractive summarisation. We therefore install bert-extractive-summarizer. This tool utilizes the HuggingFace Pytorch transformers library to run extractive summarizations. It works by first embedding the sentences, then running a clustering algorithm, finding the sentences that are closest to the cluster's centroids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up BERT model\n",
    "bert_model = Summarizer()\n",
    "\n",
    "# Defining function to apply to emails\n",
    "def bert_summary(text):\n",
    "    bert_text = ''.join(bert_model(text))\n",
    "    return bert_text\n",
    "\n",
    "# Applying function to emails\n",
    "emails_labelled[\"Bert\"] = emails_labelled[\"Bert\"].apply(lambda x: bert_summary(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-trained GPT-2\n",
    "\n",
    "As with BERT, we train a pre-trained BERT model emails. Within the TransformerSummarizer() wrapper function, we simply configure 'GPT2' as paramenter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up GPT-2 model\n",
    "GPT2_model = TransformerSummarizer(transformer_type=\"GPT2\",transformer_model_key=\"gpt2-medium\")\n",
    "\n",
    "# Defining function to apply to emails\n",
    "def GPT2_summary(text):\n",
    "    GPT2_text = ''.join(GPT2_model(text, min_length=50))\n",
    "    return GPT2_text\n",
    "\n",
    "# Applying function to emails\n",
    "emails_labelled[\"GPT2\"] = emails_labelled[\"GPT2\"].apply(lambda x: GPT2_summary(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning for Text Rank\n",
    "\n",
    "With the pre-trained BERT and GPT-2 models, no major data cleaning and pre-processing was necessary. Thus these steps are performed now, to prepare the data for further extraction summarisation models, namely Text Rank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining function to remove special characters\n",
    "def clean(email):\n",
    "    return re.sub(r\"^.*:\\s.*|^-.*\\n?.*-$|\\n|^>\", '', email, 0, re.MULTILINE)\n",
    "\n",
    "# Defining function to remove stop words\n",
    "def removeStopwords(sentence):\n",
    "    newSentence = \" \".join([i for i in sentence if i not in stopWords])\n",
    "    return newSentence\n",
    "\n",
    "# Defining function to present each sentence separately\n",
    "def prettySentences(sentence):\n",
    "    for s in sentence:\n",
    "        print(s)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying functions defined above to data and appending it to dataframe\n",
    "\n",
    "# 1. Applying clean function\n",
    "mydata = []\n",
    "for sentence in emails_labelled.body:\n",
    "    mydata.append(clean(sentence))\n",
    "\n",
    "# 2. Applying Stopwords and prettySentences functions\n",
    "sentences = []\n",
    "for sentence in mydata:\n",
    "    sentences.append(sent_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using 100 dimension version of Glove Embedding\n",
    "wordEmbeddings = {}\n",
    "with open (\"/project/glove.6B.100d.txt\", encoding = 'utf - 8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        key = values[0]\n",
    "        wordEmbeddings[key] = np.asarray(values[1:], dtype = 'float32')\n",
    "        \n",
    "cleanSentences = []\n",
    "\n",
    "# sentence formatting and removal of stop words\n",
    "for email in sentences:\n",
    "        email = [re.sub(r\"[^a-zA-Z]\", \" \", s, 0, re.MULTILINE) for s in email]\n",
    "        email = [re.sub(r\"\\s+\", \" \", s, 0, re.MULTILINE) for s in email]\n",
    "        cleanSentences.append([s.lower() for s in email])\n",
    "        \n",
    "stopWords = stopwords.words('english')\n",
    "\n",
    "for i in range(len(cleanSentences)):\n",
    "    cleanSentences[i] = [removeStopwords(r.split()) for r in cleanSentences[i]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Rank\n",
    "\n",
    "TextRank is an extractive and unsupervised text summarization technique. It ranks sentences along their importance, by assigning them a similiarity score, which are then stored in a square matrix.\n",
    "Put differently, a vector representation, word embedding, is established for each sentence. Similarities between sentence vectors are then calculated and stored in a matrix. The similarity matrix is then converted into a graph, with sentences as vertices and similarity scores as edges, for sentence rank calculation. Finally, a certain number of top-ranked sentences form the final summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating vector representations\n",
    "sentenceVectors = []\n",
    "for email in cleanSentences:\n",
    "    temp = []\n",
    "    for s in email:\n",
    "        if len(s) != 0:\n",
    "            v = sum([ wordEmbeddings.get(w, np.zeros((100,))) for w in s.split()])/(len(s.split()) + 0.001)\n",
    "        else:\n",
    "            v = np.zeros((100,))\n",
    "        temp.append(v)\n",
    "    sentenceVectors.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating similarity stores and storing them to matrix\n",
    "similarityMatrix = []\n",
    "for i in range(len(cleanSentences)):\n",
    "    email = cleanSentences[i]\n",
    "    temp = np.zeros((len(email), len(email)))\n",
    "    j_range = temp.shape[0]\n",
    "    k_range = temp.shape[1]\n",
    "    for j in range(j_range):\n",
    "        for k in range(k_range):\n",
    "            if j != k:\n",
    "                temp[j][k] = cosine_similarity(sentenceVectors[i][j].reshape(1, 100),\n",
    "                                                             sentenceVectors[i][k].reshape(1, 100))[0][0]\n",
    "    similarityMatrix.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarity matrix converted into a graph, with sentences as vertices and similarity scores as edges\n",
    "scores = []\n",
    "for i in similarityMatrix:\n",
    "    nxGraph= nx.from_numpy_array(i)\n",
    "    scores.append(nx.pagerank_numpy(nxGraph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ranking top sentences\n",
    "rankedSentences = []\n",
    "\n",
    "for i in range(len(scores)):\n",
    "    rankedSentences.append(sorted(((scores[i][j],s) for j,s in enumerate(sentences[i])), reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Out of the number of sentences extracted, we only want to take 2 sentence from each email to as a maximum limit\n",
    "textrank_summarised = []\n",
    "for i in range(0,95):\n",
    "    sentence = rankedSentences[i]\n",
    "    sentence_1 = sentence[0]\n",
    "    if len(rankedSentences[i]) > 1:\n",
    "            sentence_2 = rankedSentences[i]\n",
    "            sentence_2 = sentence_2[1]\n",
    "    else:\n",
    "        sentence_2 = (\"0\",\"0\")\n",
    "    textrank_summarised.append([sentence_1,sentence_2])\n",
    "    textrank_summarised\n",
    "\n",
    "textrank_table = pd.DataFrame(textrank_summarised)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rouge Score\n",
    "\n",
    "ROUGE stands for Recall-Oriented Understudy for Gisting Evaluation and is a set of metrics for evaluating automatic summarization of texts as well as machine translations. We use it by comparing the automatically produced summaries from BERT and GPT-2 against our set manually produced reference summaries. Simply put, recall (in the context of ROUGE) refers to how much of the reference summary the system summary is recovering or capturing (freeCodeCamp,2017). <br>\n",
    "\n",
    "Due to the short nature of our emails, we make use of ROUGE-1, which refers to the overlap of unigrams, single words, between the system summary and reference summary. <br>\n",
    "\n",
    "In the context of ROUGE, while precision measures how much of the system summary was in fact relevant or needed, recall refers to how much of the reference summary the system summary is recovering or capturing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up ROUGE scorer\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1'], use_stemmer=True)\n",
    "\n",
    "# Defining function to implement on entire body\n",
    "def rouge_score(text,text2):\n",
    "    GPT_scores = scorer.score(text1,text2)\n",
    "    rouge_s = GPT_scores.get(\"rouge1\")\n",
    "    return rouge_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>Bert</th>\n",
       "      <th>bert precision</th>\n",
       "      <th>bert recall</th>\n",
       "      <th>bert fmeasure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Global Gas Pipeline group is looking to tr...</td>\n",
       "      <td>The Global Gas Pipeline group is looking to tr...</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.482759</td>\n",
       "      <td>0.528302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IEP Team,\\nAttached is a revised January CalJo...</td>\n",
       "      <td>IEP Team,\\nAttached is a revised January CalJo...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.206897</td>\n",
       "      <td>0.342857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\n\\tAt your earliest convenience, please send ...</td>\n",
       "      <td>At your earliest convenience, please send me c...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I ran a redline from the last version I had el...</td>\n",
       "      <td>I ran a redline from the last version I had el...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.628571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>---------------------- Forwarded by Daren J Fa...</td>\n",
       "      <td>---------------------- Forwarded by Daren J Fa...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.120690</td>\n",
       "      <td>0.215385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Hey John &amp; Angie, Have you heard the news from...</td>\n",
       "      <td>Hey John &amp; Angie, Have you heard the news from...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>You have received this e-mail from South Texas...</td>\n",
       "      <td>You have received this e-mail from South Texas...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Start Date: 4/22/01; HourAhead hour: 11;  No a...</td>\n",
       "      <td>Start Date: 4/22/01; HourAhead hour: 11;  No a...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Jeff - per your instructions here's the list o...</td>\n",
       "      <td>Jeff - per your instructions here's the list o...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>0.139535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>----- Forwarded by Gayla E Seiter/ENRON_DEVELO...</td>\n",
       "      <td>----- Forwarded by Gayla E Seiter/ENRON_DEVELO...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.048780</td>\n",
       "      <td>0.093023</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                body  \\\n",
       "0  The Global Gas Pipeline group is looking to tr...   \n",
       "1  IEP Team,\\nAttached is a revised January CalJo...   \n",
       "2  \\n\\tAt your earliest convenience, please send ...   \n",
       "3  I ran a redline from the last version I had el...   \n",
       "4  ---------------------- Forwarded by Daren J Fa...   \n",
       "5  Hey John & Angie, Have you heard the news from...   \n",
       "6  You have received this e-mail from South Texas...   \n",
       "7  Start Date: 4/22/01; HourAhead hour: 11;  No a...   \n",
       "8  Jeff - per your instructions here's the list o...   \n",
       "9  ----- Forwarded by Gayla E Seiter/ENRON_DEVELO...   \n",
       "\n",
       "                                                Bert  bert precision  \\\n",
       "0  The Global Gas Pipeline group is looking to tr...        0.583333   \n",
       "1  IEP Team,\\nAttached is a revised January CalJo...        1.000000   \n",
       "2  At your earliest convenience, please send me c...        1.000000   \n",
       "3  I ran a redline from the last version I had el...        1.000000   \n",
       "4  ---------------------- Forwarded by Daren J Fa...        1.000000   \n",
       "5  Hey John & Angie, Have you heard the news from...        0.000000   \n",
       "6  You have received this e-mail from South Texas...        0.000000   \n",
       "7  Start Date: 4/22/01; HourAhead hour: 11;  No a...        0.000000   \n",
       "8  Jeff - per your instructions here's the list o...        1.000000   \n",
       "9  ----- Forwarded by Gayla E Seiter/ENRON_DEVELO...        1.000000   \n",
       "\n",
       "   bert recall  bert fmeasure  \n",
       "0     0.482759       0.528302  \n",
       "1     0.206897       0.342857  \n",
       "2     0.333333       0.500000  \n",
       "3     0.458333       0.628571  \n",
       "4     0.120690       0.215385  \n",
       "5     0.000000       0.000000  \n",
       "6     0.000000       0.000000  \n",
       "7     0.000000       0.000000  \n",
       "8     0.075000       0.139535  \n",
       "9     0.048780       0.093023  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Applying ROUGE score to BERT\n",
    "bert_score_list = []\n",
    "for x in range(0,len(emails_labelled)):\n",
    "    bert_scores = scorer.score(emails_labelled[\"Bert\"][x],emails_labelled[\"summary_label\"][x])\n",
    "    rouge_s = bert_scores.get(\"rouge1\")\n",
    "    bert_score_list.append(rouge_s)\n",
    "    \n",
    "    \n",
    "# create a data frame with all the rouge score generated \n",
    "bert_score_list = pd.DataFrame(bert_score_list)\n",
    "bert_score_list = bert_score_list.rename(columns = {\"precision\":\"bert precision\", \"recall\":\"bert recall\", \"fmeasure\":\"bert fmeasure\"})\n",
    "\n",
    "bert_score_list = bert_score_list.join(emails_labelled[\"body\"])\n",
    "bert_score_list = bert_score_list.join(emails_labelled[\"Bert\"])\n",
    "\n",
    "bert_score_list = bert_score_list[[\"body\",\"Bert\",\"bert precision\",\"bert recall\",\"bert fmeasure\"]]\n",
    "bert_score_list.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of GPT2 summary with more than 0.6 fmeasure\n",
    "bert_count = bert_score_list[bert_score_list[\"bert fmeasure\"]>0.6].count()\n",
    "bert_accept = bert_count[\"bert fmeasure\"]\n",
    "bert_reject = len(emails_labelled) - bert_count[\"bert fmeasure\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>GPT2</th>\n",
       "      <th>GPT2 precision</th>\n",
       "      <th>GPT2 recall</th>\n",
       "      <th>GPT2 fmeasure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Global Gas Pipeline group is looking to tr...</td>\n",
       "      <td>The Global Gas Pipeline group is looking to tr...</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.538462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IEP Team,\\nAttached is a revised January CalJo...</td>\n",
       "      <td>IEP Team,\\nAttached is a revised January CalJo...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.352941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\n\\tAt your earliest convenience, please send ...</td>\n",
       "      <td>At your earliest convenience, please send me c...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I ran a redline from the last version I had el...</td>\n",
       "      <td>I ran a redline from the last version I had el...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.628571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>---------------------- Forwarded by Daren J Fa...</td>\n",
       "      <td>---------------------- Forwarded by Daren J Fa...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.120690</td>\n",
       "      <td>0.215385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Hey John &amp; Angie, Have you heard the news from...</td>\n",
       "      <td>Hey John &amp; Angie, Have you heard the news from...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>You have received this e-mail from South Texas...</td>\n",
       "      <td>You have received this e-mail from South Texas...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Start Date: 4/22/01; HourAhead hour: 11;  No a...</td>\n",
       "      <td>Start Date: 4/22/01; HourAhead hour: 11;  No a...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Jeff - per your instructions here's the list o...</td>\n",
       "      <td>Jeff - per your instructions here's the list o...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>0.139535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>----- Forwarded by Gayla E Seiter/ENRON_DEVELO...</td>\n",
       "      <td>----- Forwarded by Gayla E Seiter/ENRON_DEVELO...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.048780</td>\n",
       "      <td>0.093023</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                body  \\\n",
       "0  The Global Gas Pipeline group is looking to tr...   \n",
       "1  IEP Team,\\nAttached is a revised January CalJo...   \n",
       "2  \\n\\tAt your earliest convenience, please send ...   \n",
       "3  I ran a redline from the last version I had el...   \n",
       "4  ---------------------- Forwarded by Daren J Fa...   \n",
       "5  Hey John & Angie, Have you heard the news from...   \n",
       "6  You have received this e-mail from South Texas...   \n",
       "7  Start Date: 4/22/01; HourAhead hour: 11;  No a...   \n",
       "8  Jeff - per your instructions here's the list o...   \n",
       "9  ----- Forwarded by Gayla E Seiter/ENRON_DEVELO...   \n",
       "\n",
       "                                                GPT2  GPT2 precision  \\\n",
       "0  The Global Gas Pipeline group is looking to tr...        0.583333   \n",
       "1  IEP Team,\\nAttached is a revised January CalJo...        1.000000   \n",
       "2  At your earliest convenience, please send me c...        1.000000   \n",
       "3  I ran a redline from the last version I had el...        1.000000   \n",
       "4  ---------------------- Forwarded by Daren J Fa...        1.000000   \n",
       "5  Hey John & Angie, Have you heard the news from...        0.000000   \n",
       "6  You have received this e-mail from South Texas...        0.000000   \n",
       "7  Start Date: 4/22/01; HourAhead hour: 11;  No a...        0.000000   \n",
       "8  Jeff - per your instructions here's the list o...        1.000000   \n",
       "9  ----- Forwarded by Gayla E Seiter/ENRON_DEVELO...        1.000000   \n",
       "\n",
       "   GPT2 recall  GPT2 fmeasure  \n",
       "0     0.500000       0.538462  \n",
       "1     0.214286       0.352941  \n",
       "2     0.333333       0.500000  \n",
       "3     0.458333       0.628571  \n",
       "4     0.120690       0.215385  \n",
       "5     0.000000       0.000000  \n",
       "6     0.000000       0.000000  \n",
       "7     0.000000       0.000000  \n",
       "8     0.075000       0.139535  \n",
       "9     0.048780       0.093023  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Applying ROUGE score to GPT-2\n",
    "GPT2_score_list = []\n",
    "for x in range(0,len(emails_labelled)):\n",
    "    GPT_scores = scorer.score(emails_labelled[\"GPT2\"][x],emails_labelled[\"summary_label\"][x])\n",
    "    rouge_s = GPT_scores.get(\"rouge1\")\n",
    "    GPT2_score_list.append(rouge_s)\n",
    "    \n",
    "# create a data frame with all the rouge score generated     \n",
    "GPT2_score_list = pd.DataFrame(GPT2_score_list)\n",
    "GPT2_score_list = GPT2_score_list.rename(columns = {\"precision\":\"GPT2 precision\", \"recall\":\"GPT2 recall\", \"fmeasure\":\"GPT2 fmeasure\"})\n",
    "\n",
    "GPT2_score_list = GPT2_score_list.join(emails_labelled[\"body\"])\n",
    "GPT2_score_list = GPT2_score_list.join(emails_labelled[\"GPT2\"])\n",
    "GPT2_score_list = GPT2_score_list[[\"body\",\"GPT2\",\"GPT2 precision\",\"GPT2 recall\",\"GPT2 fmeasure\"]]\n",
    "GPT2_score_list.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of GPT2 summary with more than 0.6 fmeasure\n",
    "GPT2_count = GPT2_score_list[GPT2_score_list[\"GPT2 fmeasure\"]>0.6].count()\n",
    "GPT2_accept = GPT2_count[\"GPT2 fmeasure\"]\n",
    "GPT2_reject = len(emails_labelled) - GPT2_count[\"GPT2 fmeasure\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>textrank_summary</th>\n",
       "      <th>textrank precision</th>\n",
       "      <th>textrank recall</th>\n",
       "      <th>textrank fmeasure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Global Gas Pipeline group is looking to tr...</td>\n",
       "      <td>The Initial products are expected to be day ah...</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.341463</td>\n",
       "      <td>0.430769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IEP Team,\\nAttached is a revised January CalJo...</td>\n",
       "      <td>However, since the deadline has not changed (a...</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.055556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\n\\tAt your earliest convenience, please send ...</td>\n",
       "      <td>\\tAt your earliest convenience, please send me...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.476190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I ran a redline from the last version I had el...</td>\n",
       "      <td>I've forwarded the current version to Herman f...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.407407</td>\n",
       "      <td>0.578947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>---------------------- Forwarded by Daren J Fa...</td>\n",
       "      <td>Thanks............Mary Solmonson12/15/99 06:55...</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.230769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Hey John &amp; Angie, Have you heard the news from...</td>\n",
       "      <td>We're pretty excited about the whole thing.Ker...</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.032967</td>\n",
       "      <td>0.063158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>You have received this e-mail from South Texas...</td>\n",
       "      <td>Friday, January 26, 2001\\t\\t\\t7:30\\t\\t\\tRegist...</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.003390</td>\n",
       "      <td>0.006579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Start Date: 4/22/01; HourAhead hour: 11;  No a...</td>\n",
       "      <td>Variances detected.Variances detected in Energ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Jeff - per your instructions here's the list o...</td>\n",
       "      <td>AirTrans AirwaysCeladon TruckingChitaqua Airli...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>----- Forwarded by Gayla E Seiter/ENRON_DEVELO...</td>\n",
       "      <td>\\tGayla E Seiter\\t11/15/2000 08:21 AM\\t\\t SEIT...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                body  \\\n",
       "0  The Global Gas Pipeline group is looking to tr...   \n",
       "1  IEP Team,\\nAttached is a revised January CalJo...   \n",
       "2  \\n\\tAt your earliest convenience, please send ...   \n",
       "3  I ran a redline from the last version I had el...   \n",
       "4  ---------------------- Forwarded by Daren J Fa...   \n",
       "5  Hey John & Angie, Have you heard the news from...   \n",
       "6  You have received this e-mail from South Texas...   \n",
       "7  Start Date: 4/22/01; HourAhead hour: 11;  No a...   \n",
       "8  Jeff - per your instructions here's the list o...   \n",
       "9  ----- Forwarded by Gayla E Seiter/ENRON_DEVELO...   \n",
       "\n",
       "                                    textrank_summary  textrank precision  \\\n",
       "0  The Initial products are expected to be day ah...            0.583333   \n",
       "1  However, since the deadline has not changed (a...            0.166667   \n",
       "2  \\tAt your earliest convenience, please send me...            1.000000   \n",
       "3  I've forwarded the current version to Herman f...            1.000000   \n",
       "4  Thanks............Mary Solmonson12/15/99 06:55...            0.857143   \n",
       "5  We're pretty excited about the whole thing.Ker...            0.750000   \n",
       "6  Friday, January 26, 2001\\t\\t\\t7:30\\t\\t\\tRegist...            0.111111   \n",
       "7  Variances detected.Variances detected in Energ...            0.000000   \n",
       "8  AirTrans AirwaysCeladon TruckingChitaqua Airli...            0.000000   \n",
       "9  \\tGayla E Seiter\\t11/15/2000 08:21 AM\\t\\t SEIT...            0.000000   \n",
       "\n",
       "   textrank recall  textrank fmeasure  \n",
       "0         0.341463           0.430769  \n",
       "1         0.033333           0.055556  \n",
       "2         0.312500           0.476190  \n",
       "3         0.407407           0.578947  \n",
       "4         0.133333           0.230769  \n",
       "5         0.032967           0.063158  \n",
       "6         0.003390           0.006579  \n",
       "7         0.000000           0.000000  \n",
       "8         0.000000           0.000000  \n",
       "9         0.000000           0.000000  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Applying ROUGE score to TextRank\n",
    "# Since sentence is separated into numbers of list, we need to combine the 2 sentences extract into 1 sentence and put it into one column\n",
    "textrank_table[\"textrank_summary\"] = \"\"\n",
    "for i in range(0,len(textrank_table)):\n",
    "    list_sentence = []\n",
    "    x = textrank_table[0][i][1]\n",
    "    y = textrank_table[1][i][1]\n",
    "    sentence_combined = x + y\n",
    "    list_sentence.append(sentence_combined)\n",
    "    textrank_table[\"textrank_summary\"][i]=sentence_combined\n",
    "    \n",
    "#text rank rouge score \n",
    "textrank_score_list = []\n",
    "for x in range(0,len(emails_labelled)):\n",
    "    textrank_scores = scorer.score(textrank_table[\"textrank_summary\"][x],emails_labelled[\"summary_label\"][x])\n",
    "    rouge_s = textrank_scores.get(\"rouge1\")\n",
    "    textrank_score_list.append(rouge_s)\n",
    "    \n",
    "textrank_score_list = pd.DataFrame(textrank_score_list)\n",
    "textrank_score_list = textrank_score_list.rename(columns = {\"precision\":\"textrank precision\", \"recall\":\"textrank recall\", \"fmeasure\":\"textrank fmeasure\"})\n",
    "textrank_score_list = textrank_score_list.join(emails_labelled[\"body\"])\n",
    "textrank_score_list = textrank_score_list.join(textrank_table[\"textrank_summary\"])\n",
    "textrank_score_list = textrank_score_list[[\"body\",\"textrank_summary\",\"textrank precision\",\"textrank recall\",\"textrank fmeasure\"]]\n",
    "textrank_score_list.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of text rank summary with more than 0.5 fmeasure\n",
    "textrank_count = textrank_score_list[textrank_score_list[\"textrank fmeasure\"]>0.6].count()\n",
    "textrank_accept = textrank_count[\"textrank fmeasure\"]\n",
    "textrank_reject = len(emails_labelled) - textrank_count[\"textrank fmeasure\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extractive Summarisation Overview and Limitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average GPT-2 precision: 72.48%\n",
      "Average GPT-2 recall: 26.11%\n",
      "Average GPT-2 f-measure: 32.77%\n",
      "\n",
      "\n",
      "Average GPT-2 precision: 71.50%\n",
      "Average GPT-2 recall: 26.69%\n",
      "Average GPT-2 f-measure: 33.31%\n",
      "\n",
      "\n",
      "Average TextRank precision: 64.81%\n",
      "Average TextRank recall: 20.84%\n",
      "Average TextRank f-measure: 27.77%\n"
     ]
    }
   ],
   "source": [
    "print(\"Average GPT-2 precision: {:.2f}%\".format((bert_score_list[\"bert precision\"].mean())*100))\n",
    "print(\"Average GPT-2 recall: {:.2f}%\".format((bert_score_list[\"bert recall\"].mean())*100))\n",
    "print(\"Average GPT-2 f-measure: {:.2f}%\".format((bert_score_list[\"bert fmeasure\"].mean())*100))\n",
    "print(\"\\n\")\n",
    "print(\"Average GPT-2 precision: {:.2f}%\".format((GPT2_score_list[\"GPT2 precision\"].mean())*100))\n",
    "print(\"Average GPT-2 recall: {:.2f}%\".format((GPT2_score_list[\"GPT2 recall\"].mean())*100))\n",
    "print(\"Average GPT-2 f-measure: {:.2f}%\".format((GPT2_score_list[\"GPT2 fmeasure\"].mean())*100))\n",
    "print(\"\\n\")\n",
    "print(\"Average TextRank precision: {:.2f}%\".format((textrank_score_list[\"textrank precision\"].mean())*100))\n",
    "print(\"Average TextRank recall: {:.2f}%\".format((textrank_score_list[\"textrank recall\"].mean())*100))\n",
    "print(\"Average TextRank f-measure: {:.2f}%\".format((textrank_score_list[\"textrank fmeasure\"].mean())*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the outputs above, precision is quite high for all models, specifically BERT and GPT-2. Looking at recall and overall accuracy, f-measure, we see that the mdoels are not doing a sufficient job in summarization. As pre-trained models such as the bert-extractive-summarizer, do not allow for much finetuning, we recognise the limitations of the pre-trained extractive models. However, the models' lack of overlap with the manually generated summaries, does not imply the summaries generated are of inferior quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_table = pd.DataFrame()\n",
    "bert_list = [bert_accept, bert_reject]\n",
    "GPT2_list = [GPT2_accept, GPT2_reject]\n",
    "textrank_list = [textrank_accept, textrank_reject]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_table[\"Bert\"] = \"\"\n",
    "summary_table[\"GPT2\"] = \"\"\n",
    "summary_table[\"Textrank\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_table[\"Bert\"] = bert_list\n",
    "summary_table[\"GPT2\"] = GPT2_list\n",
    "summary_table[\"Textrank\"] = textrank_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Bert</th>\n",
       "      <th>GPT2</th>\n",
       "      <th>Textrank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Overlap &gt; 60%</th>\n",
       "      <td>15</td>\n",
       "      <td>18</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Overlap &lt; 60%</th>\n",
       "      <td>80</td>\n",
       "      <td>77</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Bert  GPT2  Textrank\n",
       "Overlap > 60%    15    18        12\n",
       "Overlap < 60%    80    77        83"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_table.rename(index = {0:\"Overlap > 60%\", 1:\"Overlap < 60%\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that GPT-2 is doing the best job in summarising, with regards to overlap with the manually generated summaries. <br>\n",
    "Looking at the scorings with regards to overlap only, however, fails to take into account the length of summaries, as for example, including the entire email body could simply result in a precision score of 1.\n",
    "\n",
    "Hence, we next compare the original body of the first email with each summary generated by the respective model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: The Global Gas Pipeline group is looking to trade gas transportation on \n",
      "EnronOnline.  The Initial products are expected to be day ahead and rest of \n",
      "the month transportation. Houston PipeLine and Northern Natural Gas Pipeline \n",
      "will be the initial pipelines posting bids and offers. The expected date of \n",
      "launch  is 8/23/00. We do not have the GTC or Product descriptions yet.  I \n",
      "will forward those to you'll as soon as I have them. In the meantime if \n",
      "you'll could please let me know if there are any legal, credit , tax, risk, \n",
      "and other concerns we should be addressing. Thanks a lot.\n",
      "\n",
      "Savita\n",
      "\n",
      "\n",
      "Manual Summary: Global Gas Pipeline group is looking to trade gas transportation on \n",
      "EnronOnline, any legal, credit , tax, risk, \n",
      "and other concerns we should be addressing\n",
      "\n",
      "\n",
      "BERT Summary: The Global Gas Pipeline group is looking to trade gas transportation on \n",
      "EnronOnline. Houston PipeLine and Northern Natural Gas Pipeline \n",
      "will be the initial pipelines posting bids and offers.\n",
      "\n",
      "\n",
      "GPT2 Summary: The Global Gas Pipeline group is looking to trade gas transportation on \n",
      "EnronOnline. The Initial products are expected to be day ahead and rest of \n",
      "the month transportation.\n",
      "\n",
      "\n",
      "Textrank Summary: The Initial products are expected to be day ahead and rest of the month transportation.In the meantime if you'll could please let me know if there are any legal, credit , tax, risk, and other concerns we should be addressing.\n"
     ]
    }
   ],
   "source": [
    "print(\"Original Sentence: {}\".format(emails_labelled[\"body\"][0]))\n",
    "print(\"\\n\")\n",
    "print(\"Manual Summary: {}\".format(emails_labelled[\"summary_label\"][0]))\n",
    "print(\"\\n\")\n",
    "print(\"BERT Summary: {}\".format(emails_labelled[\"Bert\"][0]))\n",
    "print(\"\\n\")\n",
    "print(\"GPT2 Summary: {}\".format(emails_labelled[\"GPT2\"][0]))\n",
    "print(\"\\n\")\n",
    "print(\"Textrank Summary: {}\".format(textrank_table[\"textrank_summary\"][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon inspection of the summary example above, we recognise that despite the ROUGE score being rather low, the summarizations extracted seem adequate in terms of context and length. While BERT and GPT-2 do a good job at giving context, TextRank captures the key action point the message conveys, by specifying that input on legal, credit, tax, risk and other concerns is required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the performance of each pre-trained model and Textrank model, the Rouge score was able to provide some of the limitation within extractive summarisation. In general,all model was able to extract information relevant to the manual summary (high precision). However, majority of the sentences summarised are much longer in comparison to the manual summarised sentences. Hence, increasing length of sentence will provide a better score in precision but this lowers the score in recall. And although this is a precision and recall trade off issues, we believe that increasing recall score would be a outlook for future modelling. Not only this, having a lower number of sentence will improve efficient in Enron's perspective to speed up communication. Another limitation for the pre-train model would be recognising the relevancy of shorter sentences such as \"Thanks a lot\" from the example above. This not only increase summarised sentence length but also reducing summarisation accuracy. To solve this problem, we believe that taking an abstractive approach so that we are able to re-phrase and restructure the emails' content."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Python3] *",
   "language": "python",
   "name": "conda-env-Python3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
